---
title: "Everyday statistics in 30 minutes"
author: "Peter Geelan-Small - Stats Central, UNSW"
date: "29th July, 2021"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    self_contained: false
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      beforeInit: "macros.js"
---



```{r setup, include = F}

knitr::opts_chunk$set(echo = F, fig.align = "center", 
                      fig.asp = 1,
                      echo = F, message = F, warning = F)

```




```{r xaringan-logo, echo = F}

library(xaringanExtra)

use_logo(
  image_url = "StatsCentralLogo_rmd.png",
  width = "110px",
  height = "128px",
  position = css_position(bottom = "-2.5em", left = "1em"),
  link_url = NULL,
  exclude_class = c("title-slide", "inverse", "hide_logo")
)

```




```{r echo = F, message = F}

library(RColorBrewer)
library(wordcloud)
library(tidyverse)
library(ggpubr)
library(faraway)  ## For "sumary"
library(statmod)
library(GGally)
library(plotly)
library(reshape2)  ## For "acast"
library(png)
library(kableExtra)
library(emmeans)

```


<style type="text/css">
.remark-slide-content {
  font-size: 28px;
  padding: 1em 1em 1em 1em;
}
</style>



# Background

Statistics in research

- May need statistics to get *information* from your data
- Information includes some indication of how system fits together and associated uncertainty
- Start with good design - fancy statistics can't usually fix holes in design
- At start of study, need a statistical plan of how to analyse results
- To consult the statistician after an experiment is finished is often merely to ask [them] to conduct a post mortem examination. [They] can perhaps say what the experiment died of. (Ronald Fisher, 1938. *Sankhya* 4: 14-17)

---

# Statistical plan

- Research question $\;\rightarrow\;$ objectives/hypotheses
- Objectives/hypotheses framed in terms of 
    - specific *outcome* and 
    - possible explanatory or predictor variables associated with that outcome
    
How do you know what statistical analysis methods to use?
- Depends on:
    - study design - independent data or not
    - type of outcome variable - continuous, binary, count


---

# Models - are there really so many?


- 1820s - linear regression based on normal distribution
- Early 1900s - Pearson's $\chi^2$ (chi-squared) test
- 1908 - $t$ test
- 1920s or so - analysis of variance (ANOVA) and analysis of covariance (ANCOVA)
- 1972 - generalised linear models - models based on normal, binomial, Poisson and other distributions unified


---
 
# Models - are there really so many?

Model names are an accident of history


```{r}

#words_df <- read.csv("../data/stats_tests.csv", header = F)

words_df <- read.csv("../data/stats_tests.csv", header = F)

wordcloud(words = words_df[ , 1], freq = words_df[ , 2],
          colors = brewer.pal(6, "Dark2"),
          random.color = T,
          scale = c(3, 1), rot.per = 1/4)

```



---

# Linear model: linear regression

```{r}

## Load data

load("../data/harris_data_obj.RData")

load("../data/kinoshita_data_obj.RData")

```


Harris (2017): Surgical instrument proficiency

Proficiency in instrument control: Is smooth instrument movement related to error rate in a ring-carrying exercise?

*Outcome:* No. errors per sec. (baseline) - continuous

*Predictor:* Mean jerk (change in acceleration) - continuous

*Data structure:* Independent

*Model:* Linear regression - normal distribution assumption

$$\mathrm{ErrorRate} = \beta_0 + \beta_1 \; \mathrm{MeanJerk}$$

---

# Linear model: linear regression


.pull-left[

```{r}

p_surg_err <- 
  ggplot(surg, aes(x = "", y = ERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Error per sec.") +
  theme_classic()

print(p_surg_err)


```


Some positive skewness

]

.pull-right[

```{r}

p_surg_jerk <- 
  ggplot(surg, aes(x = "", y = MeanJerk1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Mean jerk (m/s^3)") +
  theme_classic()

print(p_surg_jerk)

```

]


---

# Linear model: linear regression

.pull-left[

```{r}

# Chunk option: out.width = "50%"


p_surg_err_jerk <- 
  ggplot(surg_sub1, aes(x = MeanJerk1, y = ERRORS.SEC_RT1)) +
  geom_point() +
  labs(x = "Mean jerk (m/s^3)", y = "Errors per sec.") +
  theme_classic()

print(p_surg_err_jerk)

```

]

.pull-right[

- Linear relationship
- *Variability* of error rate increases with mean jerk

]


---

# Linear model: linear regression


```{r}

err.lm1 <- lm(ERRORS.SEC_RT1 ~ MeanJerk1, data = surg_sub1)

#anova(err.lm1)

sumary(err.lm1)

```

Can we accept this model as valid?

Check the assumptions!

*Assumptions*

- Residuals have constant variance
- Residuals are normally distributed


---

# Linear model: linear regression

.pull-left[

Assumption 1: Residuals have constant variance

- Fanning pattern suggests variance not constant

]


.pull-right[

```{r}

res_fit_err1_df <- data.frame(sres = rstandard(err.lm1),
                              fit = fitted(err.lm1))

p_res_fit_err1 <- 
  ggplot(res_fit_err1_df, aes(x = fit, y = sres)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Raw outcome") +
  theme_classic()

print(p_res_fit_err1)

```

]

---

# Linear model: linear regression


.pull-left[

Assumption 2: Residuals normally distributed

- No gross deviation apparent

]


.pull-right[

```{r}

p_qq_err1 <- 
  ggplot(res_fit_err1_df, aes(sample = sres)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  theme_classic()

print(p_qq_err1)

```

]


---

# Linear model: linear regression


.pull-left[

Address non-constant variance

- Log-transforming positively skewed outcome may be useful
- Log transformation maybe a little too strong

]


.pull-right[

```{r}

surg$logERRORS.SEC_RT1 <- log(surg$ERRORS.SEC_RT1 )

ggplot(surg, aes(x = "", y = logERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1))

```

]


---

# Linear model: linear regression

.pull-left[

No improvement!

]

.pull-right[

```{r}


err.lm2 <- lm(logERRORS.SEC_RT1 ~ MeanJerk1, data = surg_sub1)

res_fit_err2_df <- data.frame(sres = rstandard(err.lm2),
                              fit = fitted(err.lm2))


p_res_fit_err2 <- 
  ggplot(res_fit_err2_df, aes(x = fit, y = sres)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Log-transformed outcome") +
  theme_classic()

print(p_res_fit_err2)

```

]


---

# Generalised linear model - Poisson

But "errors per sec." is really a rate

- different types of regression model for different types of outcome variable
- count over given time intervals  - Poisson distribution (0 is possible value, no upper limit)
- *generalised* linear model (non-normal outcome variable)



---

# Generalised Linear model - Poisson


```{r}

err.glm1 <- glm(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                family = poisson, data = surg)

sumary(err.glm1)

```

Can we accept this model as valid?

Check the assumptions!

*Assumptions*

- Quantile residuals have constant variance (constant dispersion)
- Quantile residuals are normally distributed


---

# Generalised Linear model - Poisson

```{r}

res_fit_err_glm1_df <- data.frame(res = qresiduals(err.glm1),
                                  fit = fitted(err.glm1))

p_res_fit_err_glm1 <- 
  ggplot(res_fit_err_glm1_df, aes(x = fit, y = res)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Poisson model") +
  theme_classic()

print(p_res_fit_err_glm1)


```



---

```{r}

p_qq_err_glm1 <- 
  ggplot(res_fit_err_glm1_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Poisson model") +
  theme_classic()

print(p_qq_err_glm1)

```


---

# Generalised Linear model - Negative binomial

```{r}

err.glm2 <- glm.nb(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                   data = surg)

#summary(err.glm2)

```



```{r}

res_fit_err_glm2_df <- data.frame(res = qresiduals(err.glm2),
                                  fit = fitted(err.glm2))

p_res_fit_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(x = fit, y = res)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_res_fit_err_glm2)

```

---


```{r}

p_qq_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_qq_err_glm2)

```



---

```{r}

BIC(err.glm1)

BIC(err.glm2)

```

---

# Two independent groups

Is knot-tying time associated with watching an expert demonstration of the technique?

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (2 groups - control, expert)

*Data structure:* Independent

*Model:* Two independent sample $t$ test - normal distribution assumption

$$\mathrm{Time} = \beta_0 + \beta_1 \; \mathrm{Condition_{expert}}$$

---

# Two independent sample $t$ test

*Assumptions*

- Independence
    - the two samples are independent
    - data values within each sample are independent
- Constant variance
    - data values in each group have "same" variance 
- Normal distribution
    - data in each group is normally distributed

---

# Two independent sample $t$ test

*Assumptions*

- Judge if assumptions are satisfied using diagnostic plots
    - constant variance: box plot, residual vs. fitted value plot (often hard-wired in software)
    - normal quantile-quantile (Q-Q) plot

- We recommend you do *not* do hypothesis tests on assumptions

- Use hypothesis tests only for specific research questions


---

# Two independent groups 


```{r}

## Make data set with required data

ktime2 <- 
  surg %>%
  filter(Condition2 == "Control" | Condition2 == "Expert")

ktime2$Condition2 <- factor(ktime2$Condition2)

```


.pull-left[

```{r}

ggplot(ktime2, aes(x = "", y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "Time (s)") +
  ggtitle("Knot-tying time - raw data") +
  theme_classic()

```


]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic()

```

]

Positive skew is evident


---

# Two independent groups 


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Expert") +
  theme_classic()

```

]

Data deviates visibly from normal distribution

---

# Two independent groups 

.pull-left[

- Skewness in outcome variable can distort model
- Outlying points can exert undue influence
- Log-transforming outcome variable may fix this

- Log-transformation appears successful here

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = "", y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "Time (s)") +
  ggtitle("Knot-tying time - log-tranformed data") +
  theme_classic()

```

]

---

# Two independent groups 

.pull-left[

- Variances appear equal

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic()


```

]

---

# Two independent groups 


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Expert") +
  theme_classic()

```

]

Normal distribution assumption quite well satisfied

---

# Two independent groups 

Carry out two independent sample $t$ test (data on log scale)


```{r results = F}

t.test(logKTTime ~ Condition2, data = ktime2, var.equal = T)

```


```{r results = F}

kt.lm1 <- lm(logKTTime ~ Condition2, data = ktime2)

summary(kt.lm1)

```

```{r}

kt.emm1 <- emmeans(kt.lm1, pairwise ~ Condition2)

as.data.frame(kt.emm1$emmeans) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r}

as.data.frame(kt.emm1$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```

```{r}

as.data.frame(confint(kt.emm1)$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```

---

# Two independent groups 

Carry out two independent sample $t$ test

*Conclusion*

- There is no evidence against equal group means (P = 0.85).
- A 95 % confidence interval for the difference in means *on the log scale* is (-0.23, 0.28).

*Note*

You can carry out the $t$ test as a regression model. It is a special case of regression. (In SPSS, "General Linear Model"; in R, "lm")


---

# More than two independent groups

Is knot-tying time associated with watching an expert demonstration of the technique? Compare the control group against each other group.

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (4 groups - control, novice, mixed, expert)

*Data structure:* Independent

*Model:* Analysis of variance - normal distribution assumption


Multiple two-sample $t$ tests? - No! 

---

# ANOVA: More than 2 independent groups 

*Assumptions*

- Independence
    - data values are independent
- Constant variance
    - residuals have constant variance 
    - assess with residuals vs. fitted values plot
- Normal distribution
    - residuals are normally distributed
    - assess using normal Q-Q plot
    
Residual in ANOVA = observed data value - group mean    



---

# ANOVA: More than 2 independent groups 

.pull-left[

```{r}

ggplot(surg, aes(x = Condition2, y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic()

```
]


.pull-right[

```{r}

ggplot(surg, aes(x = Condition2, y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic()

```


]

---

# Outline

Continuous outcome - normal distribn

Mean jerk - *plot data*

Compare baseline to post-observation

Data not independent

Mean jerk - *plot data*

- categorical predictor - two times - paired t test
    - assumptions


---

# Outline

Continuous outcome - normal distribn

Mean jerk - *plot data*

Compare baseline to post-observation to retention

Data not independent

Mean jerk - *plot data*

- categorical predictor - three times - mixed model
    - assumptions

---

# Outline

Categorical outcome

Independent data

Knot-tying skill (sat or unsat) - - *plot data*

Is there an association with condition?

Pearson's $\chi^2$ test
- assumptions

---

# Outline

Kinoshita

POD

Binary outcome

Is there an association between NLR </>= 3 ad POD?

Pearson's $\chi^2$ test
- assumptions

Can't estimate parameter or use controlling variables


---

# Outline

Kinoshita

POD

Binary outcome

- categorical predictor - NLR
- controlling variable - ACCI


---

# Outline


ICDSC score - 1, 2, ... , 6

"Quantitative" outcome - or is it?

Predictor - NLR - continuous or categorical (by interval)?

Normal assumption untenable

- non-parametric method - Kruskal-Wallis
    - compare NLR levels against lowest (Dunn's test)
    
- ordered categorical variable - ordinal logistic regression
    - NLR categories
    - NLR continuous
    
    
  
---

# References





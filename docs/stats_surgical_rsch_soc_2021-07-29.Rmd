---
title: "Everyday statistics in 30 minutes"
author: "Peter Geelan-Small - Stats Central, UNSW"
date: "29th July, 2021"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    self_contained: false
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      beforeInit: "macros.js"
---



```{r setup, include = F}

knitr::opts_chunk$set(echo = F, fig.align = "center", 
                      fig.asp = 1,
                      echo = F, message = F, warning = F)

```




```{r xaringan-logo, echo = F}

library(xaringanExtra)

use_logo(
  image_url = "StatsCentralLogo_rmd.png",
  width = "110px",
  height = "128px",
  position = css_position(bottom = "-2.5em", left = "1em"),
  link_url = NULL,
  exclude_class = c("title-slide", "inverse", "hide_logo")
)

```




```{r echo = F, message = F}

library(RColorBrewer)
library(wordcloud)
library(tidyverse)
library(ggpubr)
library(faraway)  ## For "sumary"
library(statmod)
library(lme4)
library(lmerTest)
library(GGally)
library(plotly)
library(reshape2)  ## For "acast"
library(png)
library(kableExtra)
library(emmeans)

```


<style type="text/css">
.remark-slide-content {
  font-size: 28px;
  padding: 1em 1em 1em 1em;
}
</style>



# Background

Statistics in research

- May need statistics to get *information* from your data
- Information includes some indication of how system fits together and associated uncertainty
- Start with good design - fancy statistics can't usually fix holes in design
- At start of study, need a statistical plan of how to analyse results
- To consult the statistician after an experiment is finished is often merely to ask [them] to conduct a post mortem examination. [They] can perhaps say what the experiment died of. (Ronald Fisher, 1938. *Sankhya* 4: 14-17)

---

# Statistical plan

- Research question $\;\rightarrow\;$ objectives/hypotheses
- Objectives/hypotheses framed in terms of 
    - specific *outcome* and 
    - possible explanatory or predictor variables associated with that outcome
    
How do you know what statistical analysis methods to use?
- Depends on:
    - study design - independent data or not
    - type of outcome variable - continuous, binary, count


---

# Models - are there really so many?


- 1820s - linear regression based on normal distribution
- Early 1900s - Pearson's $\chi^2$ (chi-squared) test
- 1908 - $t$ test
- 1920s or so - analysis of variance (ANOVA) and analysis of covariance (ANCOVA)
- 1972 - generalised linear models - models based on normal, binomial, Poisson and other distributions unified


---
 
# Models - are there really so many?

Model names are an accident of history


```{r}

#words_df <- read.csv("../data/stats_tests.csv", header = F)

words_df <- read.csv("../data/stats_tests.csv", header = F)

wordcloud(words = words_df[ , 1], freq = words_df[ , 2],
          colors = brewer.pal(6, "Dark2"),
          random.color = T,
          scale = c(3, 1), rot.per = 1/4)

```



---

# Linear model: linear regression

```{r}

## Load data

load("../data/harris_data_obj.RData")

load("../data/kinoshita_data_obj.RData")

```


Harris (2017): Surgical instrument proficiency

Proficiency in instrument control: Is smooth instrument movement related to error rate in a ring-carrying exercise?

*Outcome:* No. errors per sec. (baseline) - continuous

*Predictor:* Mean jerk (change in acceleration) - continuous

*Data structure:* Independent

*Model:* Linear regression - normal distribution assumption

$$\mathrm{ErrorRate} = \beta_0 + \beta_1 \; \mathrm{MeanJerk}$$

---

# Linear model: linear regression


.pull-left[

```{r}

p_surg_err <- 
  ggplot(surg, aes(x = "", y = ERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Error per sec.") +
  theme_classic()

print(p_surg_err)


```


Some positive skewness

]

.pull-right[

```{r}

p_surg_jerk <- 
  ggplot(surg, aes(x = "", y = MeanJerk1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Mean jerk (m/s^3)") +
  theme_classic()

print(p_surg_jerk)

```

]


---

# Linear model: linear regression

.pull-left[

```{r}

# Chunk option: out.width = "50%"


p_surg_err_jerk <- 
  ggplot(surg_sub1, aes(x = MeanJerk1, y = ERRORS.SEC_RT1)) +
  geom_point() +
  labs(x = "Mean jerk (m/s^3)", y = "Errors per sec.") +
  theme_classic()

print(p_surg_err_jerk)

```

]

.pull-right[

- Linear relationship
- *Variability* of error rate increases with mean jerk

]


---

# Linear model: linear regression


```{r}

err.lm1 <- lm(ERRORS.SEC_RT1 ~ MeanJerk1, data = surg_sub1)

#anova(err.lm1)

sumary(err.lm1)

```

Can we accept this model as valid?

Check the assumptions!

*Assumptions*

- Residuals have constant variance
- Residuals are normally distributed


---

# Linear model: linear regression

.pull-left[

Assumption 1: Residuals have constant variance

- Fanning pattern suggests variance not constant

]


.pull-right[

```{r}

res_fit_err1_df <- data.frame(sres = rstandard(err.lm1),
                              fit = fitted(err.lm1))

p_res_fit_err1 <- 
  ggplot(res_fit_err1_df, aes(x = fit, y = sres)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Raw outcome") +
  theme_classic()

print(p_res_fit_err1)

```

]

---

# Linear model: linear regression


.pull-left[

Assumption 2: Residuals normally distributed

- No gross deviation apparent

]


.pull-right[

```{r}

p_qq_err1 <- 
  ggplot(res_fit_err1_df, aes(sample = sres)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  theme_classic()

print(p_qq_err1)

```

]


---

# Linear model: linear regression


.pull-left[

Address non-constant variance

- Log-transforming positively skewed outcome may be useful
- Log transformation maybe a little too strong

]


.pull-right[

```{r}

surg$logERRORS.SEC_RT1 <- log(surg$ERRORS.SEC_RT1 )

ggplot(surg, aes(x = "", y = logERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1))

```

]


---

# Linear model: linear regression

.pull-left[

No improvement!

]

.pull-right[

```{r}


err.lm2 <- lm(logERRORS.SEC_RT1 ~ MeanJerk1, data = surg_sub1)

res_fit_err2_df <- data.frame(sres = rstandard(err.lm2),
                              fit = fitted(err.lm2))


p_res_fit_err2 <- 
  ggplot(res_fit_err2_df, aes(x = fit, y = sres)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Log-transformed outcome") +
  theme_classic()

print(p_res_fit_err2)

```

]


---

# Generalised linear model - Poisson

But "errors per sec." is really a rate

- different types of regression model for different types of outcome variable
- count over given time intervals  - Poisson distribution (0 is possible value, no upper limit)
- *generalised* linear model (non-normal outcome variable)



---

# Generalised Linear model - Poisson


```{r}

err.glm1 <- glm(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                family = poisson, data = surg)

sumary(err.glm1)

```

Can we accept this model as valid?

Check the assumptions!

*Assumptions*

- Quantile residuals have constant variance (constant dispersion)
- Quantile residuals are normally distributed


---

# Generalised Linear model - Poisson

```{r}

res_fit_err_glm1_df <- data.frame(res = qresiduals(err.glm1),
                                  fit = fitted(err.glm1))

p_res_fit_err_glm1 <- 
  ggplot(res_fit_err_glm1_df, aes(x = fit, y = res)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Poisson model") +
  theme_classic()

print(p_res_fit_err_glm1)


```



---

```{r}

p_qq_err_glm1 <- 
  ggplot(res_fit_err_glm1_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Poisson model") +
  theme_classic()

print(p_qq_err_glm1)

```


---

# Generalised Linear model - Negative binomial

```{r}

err.glm2 <- glm.nb(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                   data = surg)

#summary(err.glm2)

```



```{r}

res_fit_err_glm2_df <- data.frame(res = qresiduals(err.glm2),
                                  fit = fitted(err.glm2))

p_res_fit_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(x = fit, y = res)) +
  geom_point() +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_res_fit_err_glm2)

```

---


```{r}

p_qq_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_qq_err_glm2)

```



---

```{r}

BIC(err.glm1)

BIC(err.glm2)

```

---

# Two independent groups

Is knot-tying time associated with watching an expert demonstration of the technique?

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (2 groups - control, expert)

*Data structure:* Independent

*Model:* Two independent sample $t$ test - normal distribution assumption

$$\mathrm{Time} = \beta_0 + \beta_1 \; \mathrm{Condition_{expert}}$$

---

# Two independent sample $t$ test

*Assumptions*

- Independence
    - the two samples are independent
    - data values within each sample are independent
- Constant variance
    - data values in each group have "same" variance 
- Normal distribution
    - data in each group is normally distributed

---

# Two independent sample $t$ test

*Assumptions*

- Judge if assumptions are satisfied using diagnostic plots
    - constant variance: box plot, residual vs. fitted value plot (often hard-wired in software)
    - normal quantile-quantile (Q-Q) plot

- We recommend you do *not* do hypothesis tests on assumptions

- Use hypothesis tests only for specific research questions


---

# Two independent sample $t$ test


```{r}

## Make data set with required data

ktime2 <- 
  surg %>%
  filter(Condition2 == "Control" | Condition2 == "Expert")

ktime2$Condition2 <- factor(ktime2$Condition2)

```


.pull-left[

```{r}

ggplot(ktime2, aes(x = "", y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "Time (s)") +
  ggtitle("Knot-tying time - raw data") +
  theme_classic()

```


]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic()

```

]

Positive skew is evident


---

# Two independent sample $t$ test


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Expert") +
  theme_classic()

```

]

Data deviates visibly from normal distribution

---

# Two independent sample $t$ test

.pull-left[

- Skewness in outcome variable can distort model
- Outlying points can exert undue influence
- Log-transforming outcome variable may fix this

- Log-transformation appears successful here

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = "", y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "Time (s)") +
  ggtitle("Knot-tying time - log-tranformed data") +
  theme_classic()

```

]

---

# Two independent sample $t$ test

.pull-left[

- Variances appear equal

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic()


```

]

---

# Two independent sample $t$ test


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Expert") +
  theme_classic()

```

]

Normal distribution assumption quite well satisfied

---

# Two independent sample $t$ test

Carry out two independent sample $t$ test (data on log scale)


```{r results = F}

t.test(logKTTime ~ Condition2, data = ktime2, var.equal = T)

```



```{r results = F}

kt.lm1 <- lm(KTTime ~ Condition2, data = ktime2)

summary(kt.lm1)

```



```{r results = F}

kt.lm2 <- lm(logKTTime ~ Condition2, data = ktime2)

summary(kt.lm2)

```


```{r}

kt.emm1 <- emmeans(kt.lm1, pairwise ~ Condition2)

```



```{r}

kt.emm2 <- emmeans(kt.lm2, pairwise ~ Condition2,
                   options = list(tran = "log"), type = "response")

```



```{r eval = F}

as.data.frame(kt.emm1$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(confint(kt.emm1)$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(kt.emm2$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(confint(kt.emm2)$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```




```{r}

## Display contrast for logKTTime model with t value and P value

kt.emm2a <- emmeans(kt.lm2, pairwise ~ Condition2)

as.data.frame(kt.emm2a$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(font_size = 20)

```


*Conclusion*
- There is no evidence against equal group means (p = 0.85).

*Note*

You can carry out the $t$ test as a regression model. It is a special case of regression. (In SPSS, "General Linear Model"; in R, "lm")

---

# Two independent sample $t$ test

Using the better model gives smaller estimated standard errors

*Raw data*

```{r}

as.data.frame(kt.emm1$emmeans) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 20)

```


*Log-transformed data*


```{r}

as.data.frame(kt.emm2$emmeans) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 20)

```


---

# More than two independent groups

Is knot-tying time associated with observational learning? Compare the control group against each other group.

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (4 groups - control, novice, mixed, expert)

*Data structure:* Independent

*Model:* Analysis of variance - normal distribution assumption


Multiple two-sample $t$ tests? - No! 

---

# ANOVA: More than 2 independent groups 

*Assumptions*

- Independence
    - data values are independent
- Constant variance
    - residuals have constant variance 
    - assess with residuals vs. fitted values plot
- Normal distribution
    - residuals are normally distributed
    - assess using normal Q-Q plot
    
Residual in ANOVA = observed data value - group mean    



---

# ANOVA: More than 2 independent groups 

.pull-left[

```{r}

ggplot(surg, aes(x = Condition2, y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic()

```
]


.pull-right[

```{r}

ggplot(surg, aes(x = Condition2, y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic()

```


]

---

# ANOVA: More than 2 independent groups 

```{r}

kt.lm3 <- lm(logKTTime ~ Condition2, data = surg)

anova(kt.lm3)

```

*Conclusion*

There is no evidence that knot-tying time is associated with observational learning (p = 0.95).


---

# ANOVA: More than 2 independent groups 

- As *p* is large, no comparisons with control would be made
- If *p* were small, those comparisons would be made and P values would need to be adjusted for multiple comparisons


```{r}

kt.emm3.nadj <- as.data.frame(
  emmeans(kt.lm3, trt.vs.ctrl ~ Condition2, adjust = "none")$contrasts)

kt.emm3.dun <- as.data.frame(
  emmeans(kt.lm3, trt.vs.ctrl ~ Condition2)$contrasts)

kt.emm3.nadj <- as.data.frame(kt.emm3.nadj)

kt.emm3.dun <- as.data.frame(kt.emm3.dun)

kt.emm3.nadj <- 
  kt.emm3.nadj %>%
  select(-df, -t.ratio) %>%
  rename(p.raw = p.value)

kt.emm3.comb <- data.frame(kt.emm3.nadj, 
                           p.dunnett = kt.emm3.dun$p.value)

as.data.frame(kt.emm3.comb) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 20)


```


---

# Mixed model: repeated measurements

Proficiency in instrument control: Is smooth instrument movement related to observational learning over time?

*Outcome:* Mean jerk - continuous

*Predictors:*
- Time (baseline, post-observation, retention) - categorical
- Training condition (4 groups - control, novice, mixed, expert) - categorical 

*Data structure:* non-independent

*Model:* linear mixed model with time-condition interaction - normal distribution assumption


---

# Mixed model: repeated measurements

```{r}

## Make required stacxked data set

jerk_mean1 <- 
    surg %>%
    select(Participant, MeanJerk1, MeanJerk2, MeanJerk3) %>%
    pivot_longer(!Participant, names_to = "Trial", 
    values_to = "MeanJerk")

#head(jerk_mean1) 

jerk_mean1 <- 
    jerk_mean1 %>%
    mutate(Trial2 = ifelse(Trial == "MeanJerk1", "Baseline",
                           ifelse(Trial == "MeanJerk2", "Post", 
                                  "Retention")))

#head(jerk_mean1) 

## Add "Condition2"

condit3 <- rep(surg$Condition2, each = 3)

jerk_mean2 <- data.frame(jerk_mean1, Condition2 = condit3)

#jerk_mean2[1:30, ]

jerk_mean2$Participant <- factor(jerk_mean2$Participant)

jerk_mean2$Trial2 <- factor(jerk_mean2$Trial2)

jerk_mean2$Condition2 <- factor(
    jerk_mean2$Condition2, 
    levels = c("Novice", "Expert", "Mixed", "Control"))

rm(jerk_mean1)

#summary(jerk_mean2)

```

---

# Mixed model: repeated measurements

.pull-left[

```{r}

ggplot(jerk_mean2, aes(x = Trial2, y = MeanJerk)) + 
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice 
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) + 
  labs(x = "Time", y = "Jerk (m/s^3)") +
  theme_classic()

```

]

.pull-right[


```{r}

ggplot(jerk_mean2, aes(x = Condition2, y = MeanJerk)) + 
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice 
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) + 
  labs(x = "Training condition", y = "Jerk (m/s^3)") +
  theme_classic()


```

]

---

# Mixed model: repeated measurements

.pull-left[

```{r}

ggplot(jerk_mean2, aes(x = Trial2, y = MeanJerk, colour = Condition2)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Jerk (m/s^3)", colour = "Condition") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - raw data") +
  theme_classic() 

```

]

.pull-right[

```{r}

jerk_mean2 <-
  jerk_mean2 %>%
  na.omit() %>%
  mutate(logMeanJerk = log(MeanJerk))

ggplot(jerk_mean2, aes(x = Trial2, y = logMeanJerk, colour = Condition2)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Jerk (m/s^3)", colour = "Condition") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - log-transformed data") +
  theme_classic() 

```

]

```{r eval = F}

ggplot(jerk_mean2, aes(x = Condition2, y = logMeanJerk, colour = Trial2)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Jerk (m/s^3)", colour = "Trial") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - log-transformed data") +
  theme_classic() 

```


```{r eval = F}

tapply(jerk_mean2$MeanJerk, INDEX = jerk_mean2$Trial2, sd, na.rm = T)

```


---


# Mixed model: repeated measurements


```{r}

ggplot(jerk_mean2, aes(x = Trial2, y = MeanJerk, group = Participant)) +
  geom_line() +
  labs(x = "Time", y = "Jerk (m/s^3)") +
  ggtitle("Mean jerk by group for each participant over time") +
  theme_classic() +
  facet_wrap(~ Condition2)

```



---

# Mixed model: repeated measurements


```{r}

mj.lmer1 <- lmer(MeanJerk ~ Condition2 * Trial2 + (1 | Participant),
                 data = jerk_mean2)


mj.lmer2 <- lmer(logMeanJerk ~ Condition2 * Trial2 + (1 | Participant),
                 data = jerk_mean2)

```


.pull-left[

```{r}

plot(mj.lmer1, main = "Mean jerk - resids vs fits: raw data")

```


]

.pull-right[

```{r}

plot(mj.lmer2, main = "Mean jerk - resids vs fits: log-transformed data")

```


```{r eval = F}

mj.res.dev <- residuals(mj.lmer2, type = "deviance")

mj.fit <- fitted(mj.lmer2)

mj.res.fit <- data.frame(res = mj.res.dev, res2 = sqrt(abs(mj.res.dev)), 
                         fit = mj.fit)

ggplot(mj.res.fit, aes(x = fit, y = res2)) +
  geom_point() +
  theme_classic()

```

]

```{r eval = F}

anova(mj.lmer1, type = "II")

```


```{r eval = F}

mj.lmer3 <- lmer(MeanJerk ~ Condition2 + Trial2 + (1 | Participant),
                 data = jerk_mean2)


mj.lmer4 <- lmer(logMeanJerk ~ Condition2 + Trial2 + (1 | Participant),
                 data = jerk_mean2)

```



```{r eval = F}

anova(mj.lmer3)

```


---

# Outline

Continuous outcome - normal distribn

Mean jerk - *plot data*

Compare baseline to post-observation to retention

Data not independent

Mean jerk - *plot data*

- categorical predictor - three times - mixed model
    - assumptions


---

# Outline

Continuous outcome - normal distribn

Mean jerk - *plot data*

Compare baseline to post-observation

Data not independent

Mean jerk - *plot data*

- categorical predictor - two times - paired t test
    - assumptions

---

# Outline

Categorical outcome

Independent data

Knot-tying skill (sat or unsat) - - *plot data*

Is there an association with condition?

Pearson's $\chi^2$ test
- assumptions

---

# Outline

Kinoshita

POD

Binary outcome

Is there an association between NLR </>= 3 ad POD?

Pearson's $\chi^2$ test
- assumptions

Can't estimate parameter or use controlling variables


---

# Outline

Kinoshita

POD

Binary outcome

- categorical predictor - NLR
- controlling variable - ACCI


---

# Outline


ICDSC score - 1, 2, ... , 6

"Quantitative" outcome - or is it?

Predictor - NLR - continuous or categorical (by interval)?

Normal assumption untenable

- non-parametric method - Kruskal-Wallis
    - compare NLR levels against lowest (Dunn's test)
    
- ordered categorical variable - ordinal logistic regression
    - NLR categories
    - NLR continuous
    
    
  
---

# References





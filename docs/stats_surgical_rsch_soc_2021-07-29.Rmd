---
title: "The challenge of everyday statistics in 30 minutes"
author: "Peter Geelan-Small - Stats Central, UNSW"
date: "29th July, 2021"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    self_contained: true
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      beforeInit: "macros.js"
---



```{r setup, include = F}

knitr::opts_chunk$set(echo = F, fig.align = "center", 
                      fig.asp = 1,
                      echo = F, message = F, warning = F)

```




```{r xaringan-logo, echo = F}

library(xaringanExtra)

use_logo(
  image_url = "StatsCentralLogo_rmd.png",
  width = "110px",
  height = "128px",
  position = css_position(bottom = "-2.5em", left = "1em"),
  link_url = NULL,
  exclude_class = c("title-slide", "inverse", "hide_logo")
)

```




```{r echo = F, message = F}

library(RColorBrewer)
library(wordcloud)
library(tidyverse)
library(ggpubr)
library(faraway)  ## For "sumary"
library(statmod)
library(lme4)
library(lmerTest)
library(sjPlot)
library(ggeffects)
library(GGally)
library(plotly)
library(reshape2)  ## For "acast"
library(png)
library(kableExtra)
library(emmeans)

```


<style type="text/css">
.remark-slide-content {
  font-size: 28px;
  padding: 1em 1em 1em 1em;
}
</style>



# Background

Statistics in research

- May need statistics to get *information* from your data
- Information includes relationship among system variables with associated uncertainty
- Start with good design - fancy statistics can't usually fix holes in design
- At start of study, make a statistical plan of how to analyse data
- "To consult the statistician after an experiment is finished is often merely to ask [them] to conduct a post mortem examination. [They] can perhaps say what the experiment died of." (Ronald Fisher, 1938. *Sankhya* 4: 14-17)

---

# Statistical plan

- Research question $\;\rightarrow\;$ objectives/hypotheses
- Objectives/hypotheses framed in terms of 
    - specific *outcome* and 
    - possible explanatory or predictor variables associated with that outcome
    
How do you know what statistical analysis methods to use?
- Depends on:
    - study design - data structure, independent data or not, ...
    - type of outcome variable - quantitative (continuous, proportion, count), categorical (binary, ordered, nominal)


---

# Models - are there really so many?


- 1820s - linear regression based on normal distribution
- Early 1900s - Pearson's $\chi^2$ (chi-squared) test
- 1908 - $t$ test
- 1920s or so - analysis of variance (ANOVA) and analysis of covariance (ANCOVA)
- 1972 - generalised linear models - models based on normal, binomial, Poisson and other distributions unified


---
 
# Models - are there really so many?

Model names are an accident of history


```{r}

#words_df <- read.csv("../data/stats_tests.csv", header = F)

words_df <- read.csv("../data/stats_tests.csv", header = F)

wordcloud(words = words_df[ , 1], freq = words_df[ , 2],
          colors = brewer.pal(6, "Dark2"),
          random.color = T,
          scale = c(3, 1), rot.per = 1/4)

```


```{r}

## Load data

load("../data/harris_data_obj.RData")

load("../data/kinoshita_data_obj.RData")

```


---

# Example data and analyses

```{r}

knitr::include_graphics("harris_title.png")

```

Harris et al. (2017) https://doi.org/10.1371/journal.pone.0188233


---

# Two independent groups

**Is knot-tying time associated with watching an expert demonstration of the technique?**

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (2 groups - control, expert)

*Data structure:* Independent

*Model:* Two independent sample $t$ test - normal distribution assumption

$$\mathrm{Time} = \beta_0 + \beta_1 \; \mathrm{Condition_{expert}}$$

---

# Two independent sample $t$ test

*Assumptions*

- Independence
    - the two samples are independent
    - data values within each sample are independent
- Constant variance
    - data values in each group have "same" variance 
- Normal distribution
    - data in each group is normally distributed

---

# Two independent sample $t$ test

*Assumptions*

- Judge if assumptions are satisfied using diagnostic plots
    - constant variance: box plot, residual vs. fitted value plot (often hard-wired in software)
    - normal quantile-quantile (Q-Q) plot

- We recommend you do *not* do hypothesis tests on assumptions

- Use hypothesis tests only for specific research questions


---

# Two independent sample $t$ test


```{r}

## Make data set with required data

ktime2 <- 
  surg %>%
  filter(Condition2 == "Control" | Condition2 == "Expert")

ktime2$Condition2 <- factor(ktime2$Condition2)

```


.pull-left[

```{r}

ggplot(ktime2, aes(x = "", y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "Time (s)") +
  ggtitle("Knot-tying time - raw data") +
  theme_classic()

```


]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = KTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic()

```

]

Positive skew is evident


---

# Two independent sample $t$ test


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = KTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - raw data: Expert") +
  theme_classic()

```

]

Data deviates visibly from normal distribution

---

# Two independent sample $t$ test

.pull-left[

- Skewness in outcome variable can distort model
- Outlying points can exert undue influence
- Log-transforming outcome variable may fix this

- Log-transformation appears successful here

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = "", y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "", y = "log(Time (s))") +
  ggtitle("Knot-tying time - log-transformed data") +
  theme_classic()

```

]

---

# Two independent sample $t$ test

.pull-left[

- Variances appear equal

]


.pull-right[

```{r}

ggplot(ktime2, aes(x = Condition2, y = logKTTime)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "log(Time (s))") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic()


```

]

---

# Two independent sample $t$ test


.pull-left[

```{r}

ktime2 %>%
  filter(Condition2 == "Control") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Control") +
  theme_classic()

```

]


.pull-right[

```{r}

ktime2 %>%
  filter(Condition2 == "Expert") %>% 
  ggplot(., aes(sample = logKTTime)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Normal Q-Q plot - log-transformed data: Expert") +
  theme_classic()

```

]

Normal distribution assumption quite well satisfied

---

# Two independent sample $t$ test

Carry out two independent sample $t$ test (data on log scale)


```{r results = F}

t.test(logKTTime ~ Condition2, data = ktime2, var.equal = T)

```



```{r results = F}

kt.lm1 <- lm(KTTime ~ Condition2, data = ktime2)

summary(kt.lm1)

```



```{r results = F}

kt.lm2 <- lm(logKTTime ~ Condition2, data = ktime2)

summary(kt.lm2)

```


```{r}

kt.emm1 <- emmeans(kt.lm1, pairwise ~ Condition2)

```



```{r}

kt.emm2 <- emmeans(kt.lm2, pairwise ~ Condition2,
                   options = list(tran = "log"), type = "response")

```



```{r eval = F}

as.data.frame(kt.emm1$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(confint(kt.emm1)$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(kt.emm2$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```



```{r eval = F}

as.data.frame(confint(kt.emm2)$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling()

```




```{r}

## Display contrast for logKTTime model with t value and P value

kt.emm2a <- emmeans(kt.lm2, pairwise ~ Condition2)

as.data.frame(kt.emm2a$contrasts) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(font_size = 18)

```


*Conclusion*
- There is no evidence against equal group means (p = 0.85).

*Note*

You can carry out the $t$ test as a regression model. It is a special case of regression. (In SPSS, "General Linear Model"; in R, "lm")

---

# Two independent sample $t$ test

Using the better model gives smaller estimated standard errors

*Raw data*

```{r}

as.data.frame(kt.emm1$emmeans) %>%
  rename(Condition = Condition2) %>%
  rename(Arithmetric.mean = emmean) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```


*Log-transformed data*


```{r}

as.data.frame(kt.emm2$emmeans) %>%
  rename(Condition = Condition2) %>%
  rename(Geometric.mean = response) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```


---

# More than two independent groups

**Is knot-tying time associated with observational learning? Compare the control group against each other group.**

*Outcome:* Time (s) to tie a knot - continuous

*Predictor:* Training condition (4 groups - control, novice, mixed, expert)

*Data structure:* Independent

*Model:* Analysis of variance - normal distribution assumption


Multiple two-sample $t$ tests? - No! 

---

# More than two independent groups


Model equation for ANOVA model

```{r}

kt_mod_mat <- model.matrix(lm(KTTime ~ Condition2, 
                                        data = surg)
                           )

kt_mod_mat_df <- head(kt_mod_mat)

kt_mod_mat_df <- data.frame(Condition = surg$Condition2[1:6], kt_mod_mat_df)

kt_mod_mat_df <-
  kt_mod_mat_df %>%
  rename(Intercept = X.Intercept.) %>%
  rename(Expert = Condition2Expert) %>%
  rename(Mixed = Condition2Mixed) %>%
  rename(Novice = Condition2Novice) %>%
  kbl() %>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = T, color = "white", background = "#995533")

                            

```


$$\mathrm{Time} = \beta_0 + \beta_1 \, X_{\mathrm{Expert}} + \beta_2 \, X_{\mathrm{Mixed}} + \beta_3 \, X_{\mathrm{Novice}}$$
The *X* variables take the value 0 or 1 to show which group an observation is in.

They are "indicator variables" or "dummy variables"


---



# ANOVA: More than 2 independent groups 

*Assumptions*

- Independence
    - data values are independent
- Constant variance
    - residuals have constant variance 
    - assess with residuals vs. fitted values plot
- Normal distribution
    - residuals are normally distributed
    - assess using normal Q-Q plot
    
Residual in ANOVA = observed data value - group mean    



---

# ANOVA: More than 2 independent groups 

.pull-left[

```{r}

ggplot(surg, aes(x = Condition2, y = KTTime, colour = Condition2)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - raw data") +
  theme_classic() +
  theme(legend.position = "None")


```
]


.pull-right[

```{r}

ggplot(surg, aes(x = Condition2, y = logKTTime, colour = Condition2)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) +
  labs(x = "Condition", y = "Time (s)") +
  ggtitle("Knot-tying time by group - log-transformed data") +
  theme_classic() +
  theme(legend.position = "None")

```


]

---

# ANOVA: More than 2 independent groups 

```{r}

kt.lm3 <- lm(logKTTime ~ Condition2, data = surg)

anova(kt.lm3)

```


*Conclusion*

There is no evidence that knot-tying time is associated with observational learning (p = 0.95).

---

# ANOVA: More than 2 independent groups 

- As *p* is large, no comparisons with control would be made
- If *p* were small, those comparisons would be made and P values would need to be adjusted for multiple comparisons


```{r}

kt.emm3.nadj <- as.data.frame(
  emmeans(kt.lm3, trt.vs.ctrl ~ Condition2, adjust = "none")$contrasts)

kt.emm3.dun <- as.data.frame(
  emmeans(kt.lm3, trt.vs.ctrl ~ Condition2)$contrasts)

kt.emm3.nadj <- as.data.frame(kt.emm3.nadj)

kt.emm3.dun <- as.data.frame(kt.emm3.dun)

kt.emm3.nadj <- 
  kt.emm3.nadj %>%
  select(-df, -t.ratio) %>%
  rename(p.raw = p.value)

kt.emm3.comb <- data.frame(kt.emm3.nadj, 
                           p.dunnett = kt.emm3.dun$p.value)

as.data.frame(kt.emm3.comb) %>%
  mutate(across(where(is.numeric), round, 2)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)


```


- There are particular methods for adjusting P values for specific types of multiple comparisons

---

# ANOVA: More than 2 independent groups 

Model equation (ANOVA is a special case of regression)

```{r}

kt.sum3 <- data.frame(summary(kt.lm3)$coef) 

kt.sum3 %>%
  rename(SE = Std..Error) %>%
  rename(p.value = Pr...t..) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)


```


$$\mathrm{Time} = \beta_0 + \beta_1 \, X_{\mathrm{Expert}} + \beta_2 \, X_{\mathrm{Mixed}} + \beta_3 \, X_{\mathrm{Novice}}$$




```{r}

kt.emm3 <- as.data.frame(emmeans(kt.lm3, ~ Condition2))

kt.emm3.mean <- kt.emm3$emmean

kt.est.coef <- data.frame(rbind(kt.lm3$coef, kt.emm3.mean))

rownames(kt.est.coef) <- c("Estimate", "Mean")

names(kt.est.coef)[1] <- "Intercept (Control)"

kt.est.coef %>%
  #rename(Intercept(Control) = X.Intercept.) %>%
  rename(Expert = Condition2Expert) %>%
  rename(Mixed = Condition2Mixed) %>%
  rename(Novice = Condition2Novice) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```



---

# Mixed model: repeated measurements

**Proficiency in instrument control: Is smooth instrument movement related to observational learning over time?**

*Outcome:* Mean jerk (change in acceleration) - continuous

*Predictors:*
- Time (baseline, post-observation, retention) - categorical
- Training condition (control, novice, mixed, expert) - categorical
- Time-Condition interaction

*Data structure:* Non-independent

*Model:* Linear mixed model - normal distribution assumption


---

# Mixed model: repeated measurements

*Model equation*

Mean jerk = overall mean + time effect + condition effect + time:condition effect + (subject effect)

- time: 2 dummy variables
- condition: 3 dummy variables
- time-condition: 6 dummy variables
- subject: accounts for grouping of data values by subject (i.e. non-independence)


```{r}

## Make required stacked data set

jerk_mean1 <- 
    surg %>%
    select(Participant, MeanJerk1, MeanJerk2, MeanJerk3) %>%
    pivot_longer(!Participant, names_to = "Trial", 
    values_to = "MeanJerk")

#head(jerk_mean1) 

jerk_mean1 <- 
    jerk_mean1 %>%
    mutate(Time = ifelse(Trial == "MeanJerk1", "Baseline",
                         ifelse(Trial == "MeanJerk2", "Post", 
                                  "Retention")))

#head(jerk_mean1) 

## Add "Condition2"

condit3 <- rep(surg$Condition2, each = 3)

#stereo3 <- rep(surg$Stereoscore, each = 3)

jerk_mean2 <- data.frame(jerk_mean1, Condition = condit3)

#jerk_mean2[1:30, ]

jerk_mean2$Participant <- factor(jerk_mean2$Participant)

jerk_mean2$Time <- factor(jerk_mean2$Time)

jerk_mean2$Condition <- factor(
    jerk_mean2$Condition, 
    levels = c( "Control", "Novice", "Mixed", "Expert"))

rm(jerk_mean1)

#summary(jerk_mean2)

```



```{r eval = F}

ggplot(jerk_mean2, aes(x = Time, y = MeanJerk, colour = Time)) + 
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice 
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) + 
  labs(x = "Time", y = "Mean jerk (m/s^3)") +
  theme_classic() +
  theme(legend.position = "None")

```



```{r eval = F}

ggplot(jerk_mean2, aes(x = Condition, y = MeanJerk, colour = Condition)) + 
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice 
  geom_jitter(position = position_jitter(width = 0.1, height = 0)) + 
  labs(x = "Training condition", y = "Mean jerk (m/s^3)") +
  theme_classic() +
  theme(legend.position = "None")


```



---

# Mixed model: repeated measurements

.pull-left[

```{r}

ggplot(jerk_mean2, aes(x = Time, y = MeanJerk, colour = Condition)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Mean jerk (m/s^3)", colour = "Condition") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - raw data") +
  theme_classic() 

```

]

.pull-right[

```{r}

jerk_mean2 <-
  jerk_mean2 %>%
  na.omit() %>%
  mutate(logMeanJerk = log(MeanJerk))

ggplot(jerk_mean2, aes(x = Time, y = logMeanJerk, colour = Condition)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Mean jerk (m/s^3)", colour = "Condition") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - log-transformed data") +
  theme_classic() 

```

]

```{r eval = F}

ggplot(jerk_mean2, aes(x = Condition, y = logMeanJerk, colour = Time)) +
  geom_boxplot(outlier.shape = NA) +  # avoid plotting outliers twice
  geom_point(alpha = 0.5, size = 1, position = position_jitterdodge()) +
  labs(x = "Time", y = "Mean jerk (m/s^3)", colour = "Trial") +
  #guides(fill = guide_legend(title = "Condition")) +
  ggtitle("Mean jerk - log-transformed data") +
  theme_classic() 

```


---


# Mixed model: repeated measurements


```{r}

ggplot(jerk_mean2, aes(x = Time, y = MeanJerk, group = Participant)) +
  geom_line(colour = "#66cc66") +
  labs(x = "Time", y = "Mean jerk (m/s^3)") +
  ggtitle("Mean jerk by group for each participant over time - spaghetti plot") +
  theme_classic() +
  facet_wrap(~ Condition)

```



---

# Mixed model: repeated measurements


```{r}

mj.mer1 <- lmer(MeanJerk ~ Condition * Time + (1 | Participant),
                 data = jerk_mean2)


mj.mer2 <- lmer(logMeanJerk ~ Condition * Time + (1 | Participant),
                 data = jerk_mean2)

```



```{r}

mj.lme1 <- lme(MeanJerk ~ Condition * Time, random = ~ 1 | Participant,
                 data = jerk_mean2)


mj.lme2 <- lme(logMeanJerk ~ Condition * Time, random = ~ 1 | Participant,
                 data = jerk_mean2)


```


.pull-left[

```{r}

#plot(mj.mer1, main = "Mean jerk - resids vs fits: raw data")

res_fit_mj_mer1_df <- data.frame(res = residuals(mj.mer1, type = "pearson"),
                                 fit = fitted(mj.mer1))

p_res_fit_mj_mer1 <- 
  ggplot(res_fit_mj_mer1_df, aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Mean jerk model - raw data") +
  theme_classic()

print(p_res_fit_mj_mer1)


```


]

.pull-right[

```{r}

#plot(mj.mer2, main = "Mean jerk - resids vs fits: log-transformed data")

res_fit_mj_mer2_df <- data.frame(res = residuals(mj.mer2, type = "pearson"),
                                 fit = fitted(mj.mer2))

p_res_fit_mj_mer2 <- 
  ggplot(res_fit_mj_mer2_df, aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Mean jerk model - log-transformed data") +
  theme_classic()

print(p_res_fit_mj_mer2)


```

]

Some improvement in stabilising variance from log transformation

```{r eval = F}

p_res_fit_mj_mer2a <- 
  ggplot(res_fit_mj_mer2_df, aes(x = fit, y = sqrt(abs(res)))) +
  geom_point() +
  labs(x = "Fitted values", y = "sqrt(abs(Standardised residuals))") +
  ggtitle("Mean jerk model - log-transformed data - scale-location") +
  theme_classic()

print(p_res_fit_mj_mer2a)

```



---


# Mixed model: repeated measurements


.pull-left[


Some deviation from normal distribution evident


]


.pull-right[
```{r}

ggplot(res_fit_mj_mer2_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Mean jerk model - log-transformed data") +
  theme_classic()


```

]


```{r eval = F}

normalityTest(res_fit_mj_mer2_df$res, test = "ad.test")

```

---

# Mixed model: repeated measurements


```{r eval = F}

## This is displayed in terms of sums of squares - a bit confusing!

mj.aov2 <- as.data.frame(anova(mj.mer2, type = "II"))

mj.aov2 %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```



```{r}

mj.aov2a <- as.data.frame(anova(mj.lme2))

mj.aov2a[!(row.names(mj.aov2a) %in% "(Intercept)"), ] %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```


Condition-Time interaction is not an active predictor (p = 0.73)


```{r}

#mj.mer3 <- lmer(MeanJerk ~ Condition + Time + (1 | Participant),
#                 data = jerk_mean2)

mj.mer4 <- lmer(logMeanJerk ~ Condition + Time + (1 | Participant),
                 data = jerk_mean2)

```



```{r}

mj.lme4 <- lme(logMeanJerk ~ Condition + Time, random = ~ 1 | Participant,
               data = jerk_mean2)

```



```{r eval = F}

mj.aov4 <- as.data.frame(anova(mj.mer4, type = "II"))

mj.aov4 %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```



```{r}

mj.aov4a <- as.data.frame(anova(mj.lme4))

mj.aov4a[!(row.names(mj.aov4a) %in% "(Intercept)"), ] %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```


Training regime is not associated with mean jerk (p = 0.30)


---

# Mixed model: repeated measurements


.pull-left[

```{r}

mj.emm2 <- emmeans(mj.mer2, ~ Condition | Time, 
                   options = list(tran = "log"), type = "response")

mj.emm2.df <- as.data.frame(mj.emm2)

ggplot(mj.emm2.df, aes(x = Time, y = response, colour = Condition)) +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2,
                    position = position_dodge(0.5)) +
  geom_line(aes(x = as.numeric(Time), y = response), 
            position = position_dodge(0.5)) +
  geom_point(position = position_dodge(0.5)) +
  labs(y = "Mean jerk (m/s^3)") +
  ggtitle("Estimated (geometric) mean jerk with 95 % confidence interval") +
  theme_classic()
         

```

]

.pull-right[

```{r}

mj.emm4 <- emmeans(mj.mer4, ~ Condition, 
                   options = list(tran = "log"), type = "response")


mj.emm4.df <- as.data.frame(mj.emm4)

ggplot(mj.emm4.df, aes(x = Condition, y = response, colour = Condition)) +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.1) +
  geom_line() +
  geom_point() +
  labs(y = "Mean jerk (m/s^3)") + 
  ggtitle("Estimated (geometric) mean jerk with 95 % confidence interval") +
  theme_classic() +
  theme(legend.position = "None")


```

]

Training had no apparent impact as task was simple (Harris et al.).



---

# Linear model: linear regression

**Proficiency in instrument control: Is smooth instrument movement related to error rate in a ring-carrying exercise?**

*Outcome:* No. errors per sec. (baseline) - continuous

*Predictor:* Mean jerk (change in acceleration) - continuous

*Data structure:* Independent

*Model:* Linear regression - normal distribution assumption

$$\mathrm{ErrorRate} = \beta_0 + \beta_1 \; \mathrm{MeanJerk}$$


---

# Linear model: linear regression


Data

```{r}

surg %>%
  select(Participant, ERRORS.SEC_RT1, RT1Errors, RT1Time, MeanJerk1) %>%
  head() %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  kbl() %>%
  kable_styling(full_width = F, font_size = 18)

```


---

# Linear model: linear regression


.pull-left[

```{r}

p_surg_err <- 
  ggplot(surg, aes(x = "", y = ERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Error per sec.") +
  theme_classic()

print(p_surg_err)


```


Some positive skewness

]

.pull-right[

```{r}

p_surg_jerk <- 
  ggplot(surg, aes(x = "", y = MeanJerk1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "Mean jerk (m/s^3)") +
  theme_classic()

print(p_surg_jerk)

```

]


---

# Linear model: linear regression

.pull-left[

```{r}

# Chunk option: out.width = "50%"


p_surg_err_jerk <- 
  ggplot(surg, aes(x = MeanJerk1, y = ERRORS.SEC_RT1)) +
  geom_point() +
  labs(x = "Mean jerk (m/s^3)", y = "Errors per sec.") +
  theme_classic()

print(p_surg_err_jerk)

```

]

.pull-right[

- Linear relationship
- *Variability* of error rate increases with mean jerk

]


---

# Linear model: linear regression


```{r}

err.lm1 <- lm(ERRORS.SEC_RT1 ~ MeanJerk1, data = surg)

#anova(err.lm1)

sumary(err.lm1)

```

Can we accept this model as valid?

Check the assumptions!

*Assumptions*

- Residuals have constant variance
- Residuals are normally distributed


---

# Linear model: linear regression

.pull-left[

Assumption 1: Residuals have constant variance

- Fanning pattern suggests variance not constant

]


.pull-right[

```{r}

res_fit_err1_df <- data.frame(sres = rstandard(err.lm1),
                              fit = fitted(err.lm1))

p_res_fit_err1 <- 
  ggplot(res_fit_err1_df, aes(x = fit, y = sres)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Raw outcome") +
  theme_classic()

print(p_res_fit_err1)

```

]

---

# Linear model: linear regression


.pull-left[

Assumption 2: Residuals normally distributed

- No gross deviation apparent

]


.pull-right[

```{r}

p_qq_err1 <- 
  ggplot(res_fit_err1_df, aes(sample = sres)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  theme_classic()

print(p_qq_err1)

```

]


```{r eval = F}

plot(err.lm1, which = 3)

```

---

# Linear model: linear regression


.pull-left[

Address non-constant variance

- Log-transforming positively skewed outcome may be useful
- Log transformation maybe a little too strong

]


.pull-right[

```{r}

surg$logERRORS.SEC_RT1 <- log(surg$ERRORS.SEC_RT1 )

ggplot(surg, aes(x = "", y = logERRORS.SEC_RT1)) +
  geom_boxplot(outlier.shape = NA, width = 0.4) +  # avoid plotting outliers twice
  geom_jitter(position = position_jitter(width = 0.1, height = 0.1)) +
  labs(x = "", y = "log(Errors per sec.)") +
  theme_classic()

```

]


---

# Linear model: linear regression

.pull-left[

Log transformation has stabilised variance to some degree

]

.pull-right[

```{r}


err.lm2 <- lm(logERRORS.SEC_RT1 ~ MeanJerk1, data = surg)

res_fit_err2_df <- data.frame(sres = rstandard(err.lm2),
                              fit = fitted(err.lm2))


p_res_fit_err2 <- 
  ggplot(res_fit_err2_df, aes(x = fit, y = sres)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Log-transformed outcome") +
  theme_classic()

print(p_res_fit_err2)

```

]

---

# Linear model: linear regression

Model with log-transformed response variable

```{r}

sumary(err.lm2)

```

---

# Generalised linear model - Poisson

But "errors per sec." is really a rate ...

- different types of regression model for different types of outcome variable
- count over given time intervals  - Poisson distribution (integer values, 0 is a possible value, no upper limit)
- *generalised* linear model (non-normal outcome variable)



---

# Generalised linear model - Poisson


```{r}

err.glm1 <- glm(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                family = poisson, data = surg)

sumary(err.glm1)

```

*Model equation*

$$\log{Y} = \beta_0 + \beta_1 X$$
$$\log{\mathrm{(No. of \; errors)}} = \beta_0 + \beta_1 \mathrm{MeanJerk} + \log{\mathrm{(Time \; period)}} $$

Must add "time period" adjustment, as each participant's time to complete was different


---

# Generalised linear model - Poisson

Check the assumptions!

*Assumptions*

- Quantile residuals have constant variance (constant dispersion)
- Quantile residuals are normally distributed
- Overdispersion not present


---

# Generalised linear model - Poisson

.pull-left[

```{r}

res_fit_err_glm1_df <- data.frame(res = qresiduals(err.glm1),
                                  fit = fitted(err.glm1))

ggplot(res_fit_err_glm1_df, aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Poisson model") +
  theme_classic()

```

]

.pull-right[


```{r}

ggplot(res_fit_err_glm1_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Poisson model") +
  theme_classic()

```

]

Constant var. and normal dist. assumptions are quite well satisfied 

---

# Errors per sec. - which model?


```{r eval = F}

err.lm2.pred <- predict(err.lm2, newdata = list(MeanJerk1 = surg$MeanJerk1))

err.glm1.pred <- predict(
  err.glm1, newdata = data.frame(MeanJerk1 = surg$MeanJerk1, 
                                RT1Time = rep(1, dim(surg)[1])),
  type = "response")

err.plot <- data.frame(
  Observed = surg$ERRORS.SEC_RT1, MeanJerk = surg$MeanJerk1,
  Gaussian = exp(err.lm2.pred), Poisson = err.glm1.pred)

err.plot.st<-
  err.plot %>%
  pivot_longer(!MeanJerk, names_to = "Response", values_to = "Errors.per.sec")

err.plot.st$Response <- factor(err.plot.st$Response,
                               levels = c("Observed", "Gaussian", "Poisson"))

ggplot(err.plot.st, aes(x = MeanJerk, y = Errors.per.sec, 
                        colour = Response)) +
  geom_point(data = err.plot.st[err.plot.st$Response == "Observed", ]) +
  geom_line(data = err.plot.st[err.plot.st$Response == "Gaussian", ]) +
  geom_line(data = err.plot.st[err.plot.st$Response == "Poisson", ]) +
  labs(Y = "Errors per sec.") +
  ggtitle("Errors per sec. - observed data with fitted values from Gaussian and Poisson models") + 
  theme_classic()


```


.pull-left[

```{r}

err.lm2.L <- lm(log(ERRORS.SEC_RT1) ~ MeanJerk1, data = surg)

X_mj <- seq(0.03, 0.1, by = 0.002)

p_lm_out <- plot(ggpredict(err.lm2.L, terms = "MeanJerk1 [X_mj]")) +
  ylim(0, 0.65) +
  labs(x = "Mean jerk (m/s^3)", y = "Errors per sec.") +
  ggtitle("Errors per sec. - fitted Gaussian model 
          using log-transformed response with 95 % prediction interval") + 
  theme_classic()

## Above plot call gives:
## Model has log-transformed response. Back-transforming predictions to original response scale. Standard errors are still on the log-scale.

print(p_lm_out)

```

]

.pull-right[

```{r}

p_pois_out <- plot(ggpredict(err.glm1, 
                             terms = c("MeanJerk1 [X_mj]", "RT1Time [1]"))) +
  ylim(0, 0.65) +
  labs(x = "Mean jerk (m/s^3)", y = "Errors per sec.") +
  ggtitle("Errors per sec. - fitted Poisson model with 95 % prediction interval") + 
  theme_classic()

print(p_pois_out)

```

]

Poisson: maybe better inferences - overdispersion not yet checked!



```{r eval = F}

# Generalised linear model - Neg. binomial

err.glm2 <- glm.nb(RT1Errors ~ MeanJerk1 + offset(log(RT1Time)),
                   data = surg)

#summary(err.glm2)

```



```{r eval = F}

X_mj <- seq(0.03, 0.1, by = 0.002)

p_nb <- plot(ggpredict(err.glm2, terms = c("MeanJerk1 [X_mj]", "RT1Time [1]")))

```



```{r eval = F}

res_fit_err_glm2_df <- data.frame(res = qresiduals(err.glm2),
                                  fit = fitted(err.glm2))

p_res_fit_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  labs(x = "Fitted values", y = "Standardised residuals") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_res_fit_err_glm2)

```




```{r eval = F}

p_qq_err_glm2 <- 
  ggplot(res_fit_err_glm2_df, aes(sample = res)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Normal quantile", y = "Sample quantile") +
  ggtitle("Negative binomial model") +
  theme_classic()

print(p_qq_err_glm2)

```




```{r eval = F}

BIC(err.glm1)

BIC(err.glm2)

```

  
---

# References




